# -*- coding: utf-8 -*-
"""Naive Bayes Classifier_GitHub.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Al1xl4yyYaaoJjcwPr-W06NfMC_UrORd
"""

import pandas as pd
import csv

import re
from nltk.tokenize import word_tokenize
from string import punctuation
from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')
nltk.download('punkt')

from ast import literal_eval
import json

"""### Get the Training and Test Data Sets"""

# training data set - hydrated data set with pos/neg label using twarc
## convert csv to dictionary
training = pd.read_csv('~/tweetDataFile.csv')
reader_train = csv.DictReader(open('~/tweetDataFile.csv'))
trainingData = []
for line in reader_train:
  line = dict(line)
  trainingData.append(line)


# test data set - tweets_random1000
## pre-processing
test = pd.read_csv('~/tweets_random1000.csv')
test = pd.DataFrame(test['extended_tweet'])
test = test.rename(columns={'extended_tweet':'text'})
test['label'] = ''
test.to_csv('~/tweets_random1000_prep.csv', index=False)

## convert csv to dictionary
reader_test = csv.DictReader(open('~/tweets_random1000_prep.csv'))
testDataSet = []
for line in reader_test:
  line = dict(line)
  testDataSet.append(line)

"""### Data Pre-Processing"""

# pre-processing tweets in the data sets
class PreProcessTweets:
  def __init__(self):
    self._stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])

  def processTweets(self, list_of_tweets):
    processedTweets = []
    for tweet in list_of_tweets:
      processedTweets.append((self._processTweet(tweet['text']),tweet['label']))
    return processedTweets

  def _processTweet(self, tweet):
    tweet = tweet.lower()  # convert text to lower-case
    tweet = re.sub('((www\.[^\s]+)|(https?://[^\s]+))', 'URL', tweet)  # remove URLs
    tweet = re.sub('@[^\s]+', 'AT_USER', tweet)  # remove usernames
    tweet = re.sub(r'#([^\s]+)', r'\1', tweet)  # remove the "#" in #hashtag
    tweet = word_tokenize(tweet)  # remove repeated characters (helloooooooo into hello)
    return [word for word in tweet if word not in self._stopwords]

# call it on both the training and test sets
tweetProcessor = PreProcessTweets()
preprocessedTrainingSet = tweetProcessor.processTweets(trainingData)
preprocessedTestSet = tweetProcessor.processTweets(testDataSet)

"""### Naive Bayes Classifier"""

# building the vocabulary
def buildVocabulary(preprocessedTrainingData):
    all_words = []

    for (words, sentiment) in preprocessedTrainingData:
        all_words.extend(words)

    wordlist = nltk.FreqDist(all_words)
    word_features = wordlist.keys()

    return word_features

# matching tweets against our vocabulary
def extract_features(tweet):
    tweet_words = set(tweet)
    features = {}
    for word in word_features:
        features['contains(%s)' % word] = (word in tweet_words)
    return features

# building our feature vector
word_features = buildVocabulary(preprocessedTrainingSet)
trainingFeatures = nltk.classify.apply_features(extract_features, preprocessedTrainingSet)

# training the classifier
NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)

# testing the model
NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in preprocessedTestSet]

# get the majority vote
if NBResultLabels.count('positive') > NBResultLabels.count('negative'):
    print("Overall Positive Sentiment")
    print("Positive Sentiment Percentage = " + str(100*NBResultLabels.count('positive')/len(NBResultLabels)) + "%")
else:
    print("Overall Negative Sentiment")
    print("Negative Sentiment Percentage = " + str(100*NBResultLabels.count('negative')/len(NBResultLabels)) + "%")