# -*- coding: utf-8 -*-
"""GitHub.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JV1t0uU4rjJNBx2eIGRDlpx58ziEshQb
"""

# Import modules
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Read in the 1000 labeled tweets data
df = pd.read_csv('~/tweets_random1000_labeled.csv')
df = df.iloc[:, 1:3]
df["labels"].replace("others", "Others", inplace=True)
df

df['labels'].unique()

print(df.labels.value_counts())

from sklearn.utils import resample

df_majority = df[df.labels=='Pure_Public_Relat']
df_minority1 = df[df.labels=='Prodct_Public_Relat']
df_minority2 = df[df.labels=='Brand_Public_Relat']
df_minority3 = df[df.labels=='Employee_Public_Relat']
df_minority4 = df[df.labels=='Produc_Brand_Public_Relat']
df_minority5 = df[df.labels=='Others']
 
# Upsample minority class
df_minority_upsampled1 = resample(df_minority1, 
                                 replace=True,     # sample with replacement
                                 n_samples=660,    # to match majority class
                                 random_state=123) # reproducible results
df_minority_upsampled2 = resample(df_minority2, 
                                 replace=True,
                                 n_samples=660,
                                 random_state=123)
df_minority_upsampled3 = resample(df_minority3, 
                                 replace=True,  
                                 n_samples=660,
                                 random_state=123)
df_minority_upsampled4 = resample(df_minority4, 
                                 replace=True,    
                                 n_samples=660,  
                                 random_state=123)
df_minority_upsampled5 = resample(df_minority5, 
                                 replace=True,   
                                 n_samples=660,  
                                 random_state=123)
 
# Combine majority class with upsampled minority class
df = pd.concat([df_majority, df_minority_upsampled1, df_minority_upsampled2, df_minority_upsampled3, df_minority_upsampled4, df_minority_upsampled5])
 
# Display new class counts
df.labels.value_counts()

# The maximum number of words to be used (most frequent)
MAX_NB_WORDS = 50000

# Max number of words in each tweet
MAX_SEQUENCE_LENGTH = 200

EMBEDDING_DIM = 200

import keras
from keras.preprocessing.text import Tokenizer

# Tokenizing
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(df['text'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

from keras.preprocessing.sequence import pad_sequences

# Padding sequences
X = tokenizer.texts_to_sequences(df['text'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

# Get dummies of all the labels
Y = pd.get_dummies(df['labels'].values)
print('Shape of label tensor:', Y.shape)
Y

from sklearn.model_selection import train_test_split

# Train and test data splitting
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 
                                                    test_size = 0.33, 
                                                    random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

from keras.models import Sequential
from keras.layers import Embedding, SpatialDropout1D, LSTM, Bidirectional, SimpleRNN, Dense # , Conv2D, MaxPool2D, Flatten
from keras.callbacks import EarlyStopping

# Construct the RNN model
model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(Bidirectional(LSTM(100, dropout=0.2, return_sequences=True, recurrent_dropout=0.2)))
model.add((SimpleRNN(64)))
model.add(Dense(64, activation='relu'))
model.add(Dense(6, activation='softmax'))
model.summary()

# Compilation
model.compile(optimizer='adam', 
              loss='categorical_crossentropy',
              metrics=['accuracy'])

es = EarlyStopping(monitor='val_loss',
                   mode = 'min',
                   patience=100,
                   verbose=1,
                   restore_best_weights=True,
                   min_delta=0.0001)
# Fit the model
history = model.fit(X_train, 
                    Y_train, 
                    epochs=200, 
                    batch_size=20,
                    validation_split=0.33,
                    callbacks=[es]
                    )

# Evaluate the model
accr = model.evaluate(X_test,Y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

# Plot the accuracy
plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Test')
plt.legend()
plt.show()

# Plot the loss
plt.title('Loss')
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Test')
plt.legend()
plt.show()

# Save the model
model.save('~/rnn_model.h5')

"""## make predictions"""

# Read in the original raw tweets data
original = pd.read_csv('~/tweet_500_original.csv')
original

# Subset only the text data for prediction
original = original['extended_tweet']
original

# Apply the previous tokenizer to the text
original_arr = tokenizer.texts_to_sequences(original) # original_arr can be the entire column if you are loading from a separate file
original_arr = pad_sequences(original_arr, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

# Predict the probabilities of each label upon each tweet record
result = model.predict(original_arr)
print(result)

# Round the probabilities to be dummies
result_rounded = np.round(result, 0)
print(result_rounded)

# Re-organize the prediction to be a pandas dataframe
new_cols = ['Brand_Public_Relat', 'Employee_Public_Relat', 'Others', 'Prodct_Public_Relat', 'Produc_Brand_Public_Relat', 'Pure_Public_Relat']
result_rounded = pd.DataFrame(result_rounded, columns=new_cols)
result_rounded

# Return the dummies to the exact labels
pred_labels = result_rounded.idxmax(axis=1)
pred_labels

# Append the predicted labels to the original dataset
original = pd.read_csv('~/tweet_500_original.csv')
original['pred_labels'] = pred_labels
original

# Save the dataset as a csv file
original.to_csv('~/tweet_500_original_predicted.csv', index=False)